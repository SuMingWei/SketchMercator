{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "import math\n",
    "import sys\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_total_size_data(algo='cm', row=3, width=4096, level=1, seed=1, count=1, flowkey='srcIP', \n",
    "              epochs=['10'], dataset='caida20180517_10_caida20180816_0.pcap', window_size=200):\n",
    "    res = []\n",
    "    for epoch in epochs:\n",
    "        \n",
    "        path = f\"../SketchPatternQuery/{algo}/{dataset}/\"\\\n",
    "                f\"{flowkey}/row_{row}_width_{width}_level_{level}_epoch_{epoch}_count_{count}_seed_{seed}/\"\n",
    "        \n",
    "        for dir in sorted(os.listdir(path)):\n",
    "            p = os.path.join(path, dir)\n",
    "            if os.path.isdir(p): \n",
    "                window_dir = \"window_\" + str(window_size)\n",
    "                full_path = os.path.join(path, dir, window_dir,'total_flow_size.txt')    \n",
    "                with open(full_path, 'r') as f:\n",
    "                    for val in f:\n",
    "                        res.append(int(val))\n",
    "            \n",
    "    return res\n",
    "\n",
    "# read_total_size_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fsd_data(algo='cm', row=3, width=4096, level=1, seed=1, count=1, flowkey='srcIP', \n",
    "              epochs=['10'], dataset='caida0517-125w_10_.pcap', window_size=200):\n",
    "    res = []\n",
    "    for epoch in epochs:\n",
    "        \n",
    "        path = f\"../SketchPatternQuery/{algo}/{dataset}/\"\\\n",
    "                f\"{flowkey}/row_{row}_width_{width}_level_{level}_epoch_{epoch}_count_{count}_seed_{seed}/\"\n",
    "        \n",
    "        for dir in sorted(os.listdir(path)):\n",
    "            p = os.path.join(path, dir)\n",
    "            if os.path.isdir(p): \n",
    "                window_dir = \"window_\" + str(window_size)\n",
    "                dynamic_full_path = os.path.join(path, dir, window_dir, \"randk_summation\")\n",
    "                \n",
    "                for file in sorted(os.listdir(dynamic_full_path)):  \n",
    "                    fsd_file = os.path.join(dynamic_full_path, file)\n",
    "                    fsd = {}\n",
    "                    with open(fsd_file, 'r') as f:\n",
    "                        for line in f:\n",
    "                            fsd[int(line.strip().split()[0])] = int(line.strip().split()[1])\n",
    "                            \n",
    "                    res.append(fsd)\n",
    "            \n",
    "    return res\n",
    "\n",
    "# read_fsd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_window_fsd_data(algo='cm', row=3, width=4096, level=1, seed=1, count=1, flowkey='srcIP', \n",
    "              epochs=['10'], dataset='caida0517-125w_10_.pcap', window_size=200):\n",
    "    res = []\n",
    "    for epoch in epochs:\n",
    "        \n",
    "        path = f\"../SketchPatternQuery/{algo}/{dataset}/\"\\\n",
    "                f\"{flowkey}/row_{row}_width_{width}_level_{level}_epoch_{epoch}_count_{count}_seed_{seed}/\"\n",
    "        \n",
    "        for dir in sorted(os.listdir(path)):\n",
    "            p = os.path.join(path, dir)\n",
    "            if os.path.isdir(p): \n",
    "                window_dir = \"window_\" + str(window_size)\n",
    "                dynamic_full_path = os.path.join(path, dir, window_dir, \"single_window_randk_summation\")\n",
    "                \n",
    "                for file in sorted(os.listdir(dynamic_full_path)):  \n",
    "                    fsd_file = os.path.join(dynamic_full_path, file)\n",
    "                    fsd = {}\n",
    "                    with open(fsd_file, 'r') as f:\n",
    "                        for line in f:\n",
    "                            if int(line.strip().split()[0]) == 0:\n",
    "                                continue\n",
    "                            fsd[int(line.strip().split()[0])] = int(line.strip().split()[1])\n",
    "                            \n",
    "                    res.append(fsd)\n",
    "            \n",
    "    return res\n",
    "\n",
    "# read_fsd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_window_gt_fsd_data(algo='cm', row=3, width=4096, level=1, seed=1, count=1, flowkey='srcIP', \n",
    "              epochs=['10'], dataset='caida0517-125w_10_.pcap', window_size=200):\n",
    "    res = []\n",
    "    for epoch in epochs:\n",
    "        \n",
    "        path = f\"../SketchPatternQuery/{algo}/{dataset}/\"\\\n",
    "                f\"{flowkey}/row_{row}_width_{width}_level_{level}_epoch_{epoch}_count_{count}_seed_{seed}/\"\n",
    "        \n",
    "        for dir in sorted(os.listdir(path)):\n",
    "            p = os.path.join(path, dir)\n",
    "            if os.path.isdir(p): \n",
    "                window_dir = \"window_\" + str(window_size)\n",
    "                dynamic_full_path = os.path.join(path, dir, window_dir, \"single_window_randk_gt_summation\")\n",
    "                \n",
    "                for file in sorted(os.listdir(dynamic_full_path)):  \n",
    "                    fsd_file = os.path.join(dynamic_full_path, file)\n",
    "                    fsd = {}\n",
    "                    with open(fsd_file, 'r') as f:\n",
    "                        for line in f:\n",
    "                            if int(line.strip().split()[0]) == 0:\n",
    "                                continue\n",
    "                            fsd[int(line.strip().split()[0])] = int(line.strip().split()[1])\n",
    "                            \n",
    "                    res.append(fsd)\n",
    "            \n",
    "    return res\n",
    "\n",
    "# read_fsd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gt_fsd_data(algo='cm', row=3, width=4096, level=1, seed=1, count=1, flowkey='srcIP', \n",
    "              epochs=['10'], dataset='caida0517-125w_10_.pcap', window_size=200):\n",
    "    res = []\n",
    "    for epoch in epochs:\n",
    "        \n",
    "        path = f\"../SketchPatternQuery/{algo}/{dataset}/\"\\\n",
    "                f\"{flowkey}/row_{row}_width_{width}_level_{level}_epoch_{epoch}_count_{count}_seed_{seed}/\"\n",
    "        \n",
    "        for dir in sorted(os.listdir(path)):\n",
    "            p = os.path.join(path, dir)\n",
    "            if os.path.isdir(p): \n",
    "                window_dir = \"window_\" + str(window_size)\n",
    "                dynamic_full_path = os.path.join(path, dir, window_dir, \"randk_gt_summation\")\n",
    "                \n",
    "                for file in sorted(os.listdir(dynamic_full_path)):  \n",
    "                    fsd_file = os.path.join(dynamic_full_path, file)\n",
    "                    fsd = {}\n",
    "                    with open(fsd_file, 'r') as f:\n",
    "                        for line in f:\n",
    "                            fsd[int(line.strip().split()[0])] = int(line.strip().split()[1])\n",
    "                            \n",
    "                    sorted_fsd = dict(sorted(fsd.items()))\n",
    "                            \n",
    "                    res.append(sorted_fsd)\n",
    "            \n",
    "    return res\n",
    "\n",
    "# read_fsd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_flow_line(key, line):\n",
    "    string_key = line.split(\") \")[0]\n",
    "    string_key += \")\"\n",
    "\n",
    "    left = line.split(\") \")[1]\n",
    "\n",
    "    left = left.replace(\"]\", \"\")\n",
    "    left = left.replace(\"[\", \"\")\n",
    "    splitted = left.split(\" \")\n",
    "    estimate = int(splitted[0])\n",
    "    \n",
    "    return string_key, estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_profiler_final_fsd_dict(algo='cm', row=3, width=4096, level=1, seed=1, count=1, flowkey='srcIP', \n",
    "                                epochs=['10']):\n",
    "    fs_freq_map = {}\n",
    "    # profiler_file = []\n",
    "    \n",
    "    profiler_folder_path = \"/home/ming/SketchMercator/pattern_detection/traffic_generator/pcap_file/\"\n",
    "    profiler_file = [\"caida0517-500w_10_.pcap\",\n",
    "                    \"caida0816-600w_10_.pcap\",\n",
    "                    \"zipf2a-150w_10_.pcap\",\n",
    "                    \"zipf2b-400w_10_.pcap\"\n",
    "                    ]\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        for file in profiler_file:\n",
    "            path = f\"../SketchPadding/{algo}/{file}/\"\\\n",
    "                    f\"{flowkey}/row_{row}_width_{width}_level_{level}_epoch_{epoch}_count_{count}_seed_{seed}/\"\n",
    "            \n",
    "            for dir in sorted(os.listdir(path)):\n",
    "                p = os.path.join(path, dir)\n",
    "                if os.path.isdir(p): \n",
    "                    flowkey_path = os.path.join(p, \"flowkey.txt\")\n",
    "                    fs_freq_map[file] = {}\n",
    "            \n",
    "                    f = open(flowkey_path)\n",
    "                    key = f.readline().strip()\n",
    "                    # print(key)\n",
    "                    for line in f:\n",
    "                        string_key, estimate = parse_flow_line(key, line.strip())\n",
    "                        fs_freq_map[file][string_key] = estimate\n",
    "                    f.close()\n",
    "            \n",
    "    return fs_freq_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf(fsd):\n",
    "    pdf_fsd = {}\n",
    "    total_flows = sum(list(fsd.values()))\n",
    "    for key, val in fsd.items():\n",
    "        pdf_fsd[key] = val/total_flows\n",
    "        \n",
    "    return pdf_fsd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profiler_sampled_fsd(profiler_fs_freq_map, randomk_key):\n",
    "    fsd = {}\n",
    "    for key in randomk_key:\n",
    "        if key in profiler_fs_freq_map.keys():\n",
    "            if profiler_fs_freq_map[key] in fsd.keys():\n",
    "                fsd[profiler_fs_freq_map[key]] += 1\n",
    "            else:\n",
    "                fsd[profiler_fs_freq_map[key]] = 1\n",
    "                \n",
    "    sorted_fsd = dict(sorted(list(fsd.items())))\n",
    "        \n",
    "    return sorted_fsd\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampled_flowkey(algo='cm', row=3, width=4096, level=1, seed=1, count=1, flowkey='srcIP', \n",
    "                            epochs=['10'], dataset = \"caida0517-125w_10_.pcap\", window_size=200, predict_time=1):\n",
    "    \n",
    "    sampled_flowkey = []\n",
    "    for epoch in epochs:\n",
    "        \n",
    "        path = f\"../SketchPadding/{algo}/{dataset}/\"\\\n",
    "                f\"{flowkey}/row_{row}_width_{width}_level_{level}_epoch_{epoch}_count_{count}_seed_{seed}/\"\n",
    "        \n",
    "        for dir in sorted(os.listdir(path)):\n",
    "            p = os.path.join(path, dir)\n",
    "            if os.path.isdir(p): \n",
    "                for l in range(0, 1):\n",
    "                    key_window_dir = '%s/level_%02d/key_window_%d/' % (p, l, window_size)\n",
    "                    if not os.path.isdir(key_window_dir):\n",
    "                        continue\n",
    "                    \n",
    "                    cnt = 0 # count for predict dist\n",
    "                    for file in sorted(os.listdir(key_window_dir)):\n",
    "                        key_list = []\n",
    "                        key_window_path = os.path.join(key_window_dir, file)\n",
    "                            \n",
    "                        f = open(key_window_path)\n",
    "                        key = f.readline().strip()\n",
    "                        for line in f:\n",
    "                            string_key, estimate = parse_flow_line(key, line.strip())\n",
    "                            key_list.append(string_key)\n",
    "                            \n",
    "                        f.close()\n",
    "                        \n",
    "                        sampled_flowkey.append(key_list)\n",
    "                        \n",
    "                        cnt += 1\n",
    "                        if predict_time != 0 and cnt >= int(predict_time*1000/window_size):\n",
    "                            break\n",
    "                        \n",
    "                              \n",
    "    return sampled_flowkey\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caida0517-500w_10_.pcap', 'caida0517-250w_10_.pcap', 'caida0517-125w_10_.pcap', 'caida0816-600w_10_.pcap', 'caida0816-300w_10_.pcap', 'caida0816-150w_10_.pcap', 'zipf2a-150w_10_.pcap', 'zipf2a-75w_10_.pcap', 'zipf2a-35w_10_.pcap', 'zipf2b-400w_10_.pcap', 'zipf2b-200w_10_.pcap', 'zipf2b-100w_10_.pcap', 'caida0517-500w_6_caida0816-600w_4.pcap', 'caida0517-500w_6_caida0816-300w_4.pcap', 'caida0517-500w_6_caida0816-150w_4.pcap', 'caida0517-250w_6_caida0816-600w_4.pcap', 'caida0517-250w_6_caida0816-300w_4.pcap', 'caida0517-250w_6_caida0816-150w_4.pcap', 'caida0517-125w_6_caida0816-600w_4.pcap', 'caida0517-125w_6_caida0816-300w_4.pcap', 'caida0517-125w_6_caida0816-150w_4.pcap', 'caida0816-600w_6_caida0517-500w_4.pcap', 'caida0816-600w_6_caida0517-250w_4.pcap', 'caida0816-600w_6_caida0517-125w_4.pcap', 'caida0816-300w_6_caida0517-500w_4.pcap', 'caida0816-300w_6_caida0517-250w_4.pcap', 'caida0816-300w_6_caida0517-125w_4.pcap', 'caida0816-150w_6_caida0517-500w_4.pcap', 'caida0816-150w_6_caida0517-250w_4.pcap', 'caida0816-150w_6_caida0517-125w_4.pcap', 'zipf2a-150w_6_zipf2b-400w_4.pcap', 'zipf2a-150w_6_zipf2b-200w_4.pcap', 'zipf2a-150w_6_zipf2b-100w_4.pcap', 'zipf2a-75w_6_zipf2b-400w_4.pcap', 'zipf2a-75w_6_zipf2b-200w_4.pcap', 'zipf2a-75w_6_zipf2b-100w_4.pcap', 'zipf2a-35w_6_zipf2b-400w_4.pcap', 'zipf2a-35w_6_zipf2b-200w_4.pcap', 'zipf2a-35w_6_zipf2b-100w_4.pcap', 'zipf2b-400w_6_zipf2a-150w_4.pcap', 'zipf2b-400w_6_zipf2a-75w_4.pcap', 'zipf2b-400w_6_zipf2a-35w_4.pcap', 'zipf2b-200w_6_zipf2a-150w_4.pcap', 'zipf2b-200w_6_zipf2a-75w_4.pcap', 'zipf2b-200w_6_zipf2a-35w_4.pcap', 'zipf2b-100w_6_zipf2a-150w_4.pcap', 'zipf2b-100w_6_zipf2a-75w_4.pcap', 'zipf2b-100w_6_zipf2a-35w_4.pcap', 'caida0517-500w_6_zipf2a-150w_4.pcap', 'caida0517-500w_6_zipf2a-75w_4.pcap', 'caida0517-500w_6_zipf2a-35w_4.pcap', 'caida0517-250w_6_zipf2a-150w_4.pcap', 'caida0517-250w_6_zipf2a-75w_4.pcap', 'caida0517-250w_6_zipf2a-35w_4.pcap', 'caida0517-125w_6_zipf2a-150w_4.pcap', 'caida0517-125w_6_zipf2a-75w_4.pcap', 'caida0517-125w_6_zipf2a-35w_4.pcap', 'caida0816-600w_6_zipf2a-150w_4.pcap', 'caida0816-600w_6_zipf2a-75w_4.pcap', 'caida0816-600w_6_zipf2a-35w_4.pcap', 'caida0816-300w_6_zipf2a-150w_4.pcap', 'caida0816-300w_6_zipf2a-75w_4.pcap', 'caida0816-300w_6_zipf2a-35w_4.pcap', 'caida0816-150w_6_zipf2a-150w_4.pcap', 'caida0816-150w_6_zipf2a-75w_4.pcap', 'caida0816-150w_6_zipf2a-35w_4.pcap', 'caida0517-500w_6_zipf2b-400w_4.pcap', 'caida0517-500w_6_zipf2b-200w_4.pcap', 'caida0517-500w_6_zipf2b-100w_4.pcap', 'caida0517-250w_6_zipf2b-400w_4.pcap', 'caida0517-250w_6_zipf2b-200w_4.pcap', 'caida0517-250w_6_zipf2b-100w_4.pcap', 'caida0517-125w_6_zipf2b-400w_4.pcap', 'caida0517-125w_6_zipf2b-200w_4.pcap', 'caida0517-125w_6_zipf2b-100w_4.pcap', 'caida0816-600w_6_zipf2b-400w_4.pcap', 'caida0816-600w_6_zipf2b-200w_4.pcap', 'caida0816-600w_6_zipf2b-100w_4.pcap', 'caida0816-300w_6_zipf2b-400w_4.pcap', 'caida0816-300w_6_zipf2b-200w_4.pcap', 'caida0816-300w_6_zipf2b-100w_4.pcap', 'caida0816-150w_6_zipf2b-400w_4.pcap', 'caida0816-150w_6_zipf2b-200w_4.pcap', 'caida0816-150w_6_zipf2b-100w_4.pcap', 'zipf2a-150w_6_caida0517-500w_4.pcap', 'zipf2a-150w_6_caida0517-250w_4.pcap', 'zipf2a-150w_6_caida0517-125w_4.pcap', 'zipf2a-150w_6_caida0816-600w_4.pcap', 'zipf2a-150w_6_caida0816-300w_4.pcap', 'zipf2a-150w_6_caida0816-150w_4.pcap', 'zipf2a-75w_6_caida0517-500w_4.pcap', 'zipf2a-75w_6_caida0517-250w_4.pcap', 'zipf2a-75w_6_caida0517-125w_4.pcap', 'zipf2a-75w_6_caida0816-600w_4.pcap', 'zipf2a-75w_6_caida0816-300w_4.pcap', 'zipf2a-75w_6_caida0816-150w_4.pcap', 'zipf2a-35w_6_caida0517-500w_4.pcap', 'zipf2a-35w_6_caida0517-250w_4.pcap', 'zipf2a-35w_6_caida0517-125w_4.pcap', 'zipf2a-35w_6_caida0816-600w_4.pcap', 'zipf2a-35w_6_caida0816-300w_4.pcap', 'zipf2a-35w_6_caida0816-150w_4.pcap', 'zipf2b-400w_6_caida0517-500w_4.pcap', 'zipf2b-400w_6_caida0517-250w_4.pcap', 'zipf2b-400w_6_caida0517-125w_4.pcap', 'zipf2b-400w_6_caida0816-600w_4.pcap', 'zipf2b-400w_6_caida0816-300w_4.pcap', 'zipf2b-400w_6_caida0816-150w_4.pcap', 'zipf2b-200w_6_caida0517-500w_4.pcap', 'zipf2b-200w_6_caida0517-250w_4.pcap', 'zipf2b-200w_6_caida0517-125w_4.pcap', 'zipf2b-200w_6_caida0816-600w_4.pcap', 'zipf2b-200w_6_caida0816-300w_4.pcap', 'zipf2b-200w_6_caida0816-150w_4.pcap', 'zipf2b-100w_6_caida0517-500w_4.pcap', 'zipf2b-100w_6_caida0517-250w_4.pcap', 'zipf2b-100w_6_caida0517-125w_4.pcap', 'zipf2b-100w_6_caida0816-600w_4.pcap', 'zipf2b-100w_6_caida0816-300w_4.pcap', 'zipf2b-100w_6_caida0816-150w_4.pcap']\n",
      "Total Pcap File Number: 120\n"
     ]
    }
   ],
   "source": [
    "## parameters\n",
    "\n",
    "caida0517 = [\"caida0517-500w\", \"caida0517-250w\", \"caida0517-125w\"]\n",
    "caida0816 = [\"caida0816-600w\", \"caida0816-300w\", \"caida0816-150w\"]\n",
    "zipf2a = [\"zipf2a-150w\", \"zipf2a-75w\", \"zipf2a-35w\"]\n",
    "zipf2b = [\"zipf2b-400w\", \"zipf2b-200w\", \"zipf2b-100w\"]\n",
    "zipf4 = [\"zipf4-60w\", \"zipf4-30w\", \"zipf4-15w\"]\n",
    "\n",
    "\n",
    "lens = [\n",
    "        # [\"5\", \"5\"],\n",
    "        [\"6\", \"4\"],\n",
    "        # [\"7\", \"3\"],\n",
    "        # [\"8\", \"2\"],\n",
    "        ]\n",
    "\n",
    "pcap_file = []\n",
    "    \n",
    "# # single dataset\n",
    "pcap_file.append(\"caida0517-500w_10_.pcap\")\n",
    "pcap_file.append(\"caida0517-250w_10_.pcap\")\n",
    "pcap_file.append(\"caida0517-125w_10_.pcap\")\n",
    "pcap_file.append(\"caida0816-600w_10_.pcap\")\n",
    "pcap_file.append(\"caida0816-300w_10_.pcap\")\n",
    "pcap_file.append(\"caida0816-150w_10_.pcap\")\n",
    "pcap_file.append(\"zipf2a-150w_10_.pcap\") \n",
    "pcap_file.append(\"zipf2a-75w_10_.pcap\") \n",
    "pcap_file.append(\"zipf2a-35w_10_.pcap\") \n",
    "pcap_file.append(\"zipf2b-400w_10_.pcap\") \n",
    "pcap_file.append(\"zipf2b-200w_10_.pcap\") \n",
    "pcap_file.append(\"zipf2b-100w_10_.pcap\") \n",
    "\n",
    "\n",
    "# # # same dist, caida\n",
    "for a in caida0517:\n",
    "    for b in caida0816:\n",
    "        for l in lens:\n",
    "            pcap_file.append(f'{a}_{l[0]}_{b}_{l[1]}.pcap')\n",
    "for a in caida0816:\n",
    "    for b in caida0517:\n",
    "        for l in lens:\n",
    "            pcap_file.append(f'{a}_{l[0]}_{b}_{l[1]}.pcap')\n",
    "\n",
    "            \n",
    "# # same dist, zipf\n",
    "for a in zipf2a:\n",
    "    for b in zipf2b:\n",
    "        for l in lens:\n",
    "            pcap_file.append(f'{a}_{l[0]}_{b}_{l[1]}.pcap')\n",
    "for a in zipf2b:\n",
    "    for b in zipf2a:\n",
    "        for l in lens:\n",
    "            pcap_file.append(f'{a}_{l[0]}_{b}_{l[1]}.pcap')\n",
    "\n",
    "# # # diff dist, caida + zipf2a\n",
    "for a in caida0517:\n",
    "    for b in zipf2a:\n",
    "        for l in lens:\n",
    "            pcap_file.append(f'{a}_{l[0]}_{b}_{l[1]}.pcap')\n",
    "for a in caida0816:\n",
    "    for b in zipf2a:\n",
    "        for l in lens:\n",
    "            pcap_file.append(f'{a}_{l[0]}_{b}_{l[1]}.pcap')\n",
    "            \n",
    "# # # diff dist, caida + zipf2b\n",
    "for a in caida0517:\n",
    "    for b in zipf2b:\n",
    "        for l in lens:\n",
    "            pcap_file.append(f'{a}_{l[0]}_{b}_{l[1]}.pcap')\n",
    "for a in caida0816:\n",
    "    for b in zipf2b:\n",
    "        for l in lens:\n",
    "            pcap_file.append(f'{a}_{l[0]}_{b}_{l[1]}.pcap')\n",
    "            \n",
    "# # # diff dist, zipf2a + caida\n",
    "for a in zipf2a:\n",
    "    for b in caida0517:\n",
    "        for l in lens:\n",
    "            pcap_file.append(f'{a}_{l[0]}_{b}_{l[1]}.pcap')\n",
    "    for b in caida0816:\n",
    "        for l in lens:\n",
    "            pcap_file.append(f'{a}_{l[0]}_{b}_{l[1]}.pcap')\n",
    "            \n",
    "# # # diff dist, zipf2b + caida\n",
    "for a in zipf2b:\n",
    "    for b in caida0517:\n",
    "        for l in lens:\n",
    "            pcap_file.append(f'{a}_{l[0]}_{b}_{l[1]}.pcap')\n",
    "    for b in caida0816:\n",
    "        for l in lens:\n",
    "            pcap_file.append(f'{a}_{l[0]}_{b}_{l[1]}.pcap')\n",
    "            \n",
    "print(pcap_file)    \n",
    "print(f'Total Pcap File Number: {len(pcap_file)}')\n",
    "# widths = [2048, 4096, 8192, 16384, 32768, 65536, 131072]\n",
    "widths = [4096]\n",
    "# widths = [1024]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict TFS changes by adjacent TFS variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfs_changes_occured(var, sec_var, n, ws=20):\n",
    "    \n",
    "    time_var = -1.0\n",
    "    time_sec_var = -1.0\n",
    "    \n",
    "    val = 0\n",
    "    max_time = -1.0\n",
    "    \n",
    "    # for each sliding window\n",
    "    for i in range(len(var) - ws + 1):\n",
    "        var_mean = np.mean(var[0+i:ws+i])\n",
    "        var_std_dev = np.std(var[0+i:ws+i])\n",
    "        \n",
    "        # check by var outliers\n",
    "        if abs(var[ws+i-1] - var_mean) > (n * var_std_dev):\n",
    "            if time_var == -1.0:\n",
    "                time_var = (1+(ws+i-1)-1)/5\n",
    "                break\n",
    "                \n",
    "    for i in range(len(sec_var) - ws + 1):            \n",
    "        sec_var_mean = np.mean(sec_var[0+i:ws+i])\n",
    "        sec_var_std_dev = np.std(sec_var[0+i:ws+i])\n",
    "        \n",
    "        # check by sec var outliers\n",
    "        if abs(sec_var[ws+i-1] - sec_var_mean) > (n * sec_var_std_dev):\n",
    "            if time_sec_var == -1.0:\n",
    "                time_sec_var = (2+(ws+i-1)-1)/5\n",
    "                val = sec_var[ws+i-1]\n",
    "                break\n",
    "                \n",
    "    if val == 0:\n",
    "        val = max(sec_var)\n",
    "        max_time = (2 + sec_var.index(val) - 1)/5\n",
    "        \n",
    "    \n",
    "    var_hit = 0\n",
    "    sec_var_hit = 0\n",
    "    \n",
    "    if time_var != -1.0:\n",
    "        var_hit = 1\n",
    "    if time_sec_var != -1.0:\n",
    "        sec_var_hit = 1\n",
    "    \n",
    "    return var_hit, time_var, sec_var_hit, time_sec_var, val, max_time\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tfs(algo='cm', row=3, width=4096, level=1, seed=1, count=1, flowkey='srcIP', \n",
    "              epochs=['10'], dataset='zipf2a_3_caida20180517_7.pcap', window_size=200, dev=3, ws=25):\n",
    "    \n",
    "    # total flow size\n",
    "    res_sum = []\n",
    "    res_var = []\n",
    "    res_sec_var = []\n",
    "    \n",
    "    answer = int(dataset.split('_')[1])\n",
    "    # print(dataset)\n",
    "    \n",
    "    res_var = [None]\n",
    "    res_sec_var = [None, None]\n",
    "            \n",
    "    res_sum = read_total_size_data(algo, row, width, level, seed, count, flowkey, epochs, dataset, window_size)\n",
    "        \n",
    "    for i in range(1, len(res_sum)):\n",
    "        res_var.append(res_sum[i] - res_sum[i-1])\n",
    "        \n",
    "    for i in range(2, len(res_var)):\n",
    "        res_sec_var.append(abs(res_var[i] - res_var[i-1]))\n",
    "            \n",
    "    \n",
    "    var_hit, time_var, sec_var_hit, time_sec_var, max_val, max_time = tfs_changes_occured(res_var[1:], res_sec_var[2:], dev, ws)\n",
    "    \n",
    "    change = 0\n",
    "    changing_time = 0.0\n",
    "    if sec_var_hit == 1:\n",
    "        change = 1\n",
    "        changing_time = time_sec_var\n",
    "\n",
    "    return change, changing_time, answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7.6, 10)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_tfs(width=widths[0], dataset=pcap_file[0], dev=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict TFS Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tfs_val(algo='cm', row=3, width=4096, level=1, seed=1, count=1, flowkey='srcIP', \n",
    "              epochs=['10'], dataset='zipf2a_3_caida20180517_7.pcap', window_size=200, start_time=30, predict_length=2):\n",
    "    \n",
    "\n",
    "    res = []\n",
    "    # sum\n",
    "    result = read_total_size_data(algo, row, width, level, seed, count, flowkey, epochs, dataset, window_size)[start_time:]\n",
    "    \n",
    "    # res.append(result)\n",
    "    \n",
    "    # variation\n",
    "    fin_res_var = [None]\n",
    "    for i in range(1, len(result)):\n",
    "        fin_res_var.append(result[i] - result[i-1])\n",
    "        \n",
    "    res.append(fin_res_var)\n",
    "    \n",
    "    # second variation\n",
    "    fin_sec_res_var = [None, None]\n",
    "    for i in range(2, len(result)):\n",
    "        fin_sec_res_var.append(abs(fin_res_var[i] - fin_res_var[i-1]))\n",
    "        \n",
    "    res.append(fin_sec_res_var)\n",
    "    \n",
    "\n",
    "    # actual_tfs = result[-1]\n",
    "    predict_tfs = sum(fin_res_var[1:min(len(fin_res_var), 1+int(predict_length*1000/window_size))]) * (10/predict_length)\n",
    "    \n",
    "    return int(predict_tfs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2907745"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_tfs_val( dataset=pcap_file[13], start_time=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict FSD changes by adjacent MRD variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_profiler_bins():\n",
    "    res = {}\n",
    "    \n",
    "    profiler_path = \"../traffic_generator/fs_dist/\"\n",
    "    profiler_fsd = []\n",
    "    for file in sorted(os.listdir(profiler_path)):\n",
    "        profiler_fsd.append(file)\n",
    "        \n",
    "    for file in profiler_fsd:\n",
    "        path = os.path.join(profiler_path, file)\n",
    "        \n",
    "        # Read file to get profilers' flow size distribution\n",
    "        fsd = {}\n",
    "        fn = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                fsd[int(line.strip().split()[0])] = int(line.strip().split()[1])\n",
    "                fn += int(line.strip().split()[1])\n",
    "                \n",
    "        # calcualte CDF\n",
    "        cdf = {}\n",
    "        culmulative_prob = 0.0\n",
    "        for fs, freq in sorted(fsd.items()):\n",
    "            culmulative_prob += (freq/fn)\n",
    "            cdf[fs] = culmulative_prob\n",
    "            \n",
    "        # get bins\n",
    "        bin = {}\n",
    "        idx = 0\n",
    "        for fs, prob in sorted(cdf.items()):\n",
    "            while idx < round(prob * 100):\n",
    "                bin[idx] = fs\n",
    "                idx += 1\n",
    "                \n",
    "        res[file[:-4]] = bin\n",
    "            \n",
    "    return res\n",
    "    \n",
    "# prepare_profiler_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantized_mapping(fsd, bins):\n",
    "    qfsd = {}\n",
    "    idx = 0\n",
    "    for fs, freq in sorted(fsd.items()):\n",
    "        while fs > bins[idx]:\n",
    "            if idx < len(bins) - 1:\n",
    "                idx += 1\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        if bins[idx] in qfsd.keys():\n",
    "            qfsd[bins[idx]] += freq\n",
    "        else:\n",
    "            qfsd[bins[idx]] = freq\n",
    "            \n",
    "        \n",
    "    return qfsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mrd_variation(mrd_var, name, window_size=200, typ='Var'):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    res.append(mrd_var)\n",
    "    \n",
    "    print(name)\n",
    "    print(mrd_var)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "        \n",
    "    for i in range(len(res)):\n",
    "        plt.plot(res[i], label=f'MRD {typ}')\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Time (sec)')\n",
    "    if typ == \"MRD\":\n",
    "        plt.ylabel('MRD')\n",
    "    else:\n",
    "        plt.ylabel('Value')\n",
    "    plt.title('%s MRD Variation' % (name))\n",
    "    # plt.axhline(10000, c=\"black\")\n",
    "    ticks = [i for i in range(int(10*1000/window_size) + 1)]\n",
    "    adjusted_ticks = [tick * (window_size / 1000) for tick in ticks[0::int(1000 / window_size)]]\n",
    "    plt.xticks(ticks[0::int(1000/window_size)], adjusted_ticks)\n",
    "    plt.legend(loc='upper left', ncol=math.ceil(len(res)/4))\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fsd_changes_occured(sec_var, n, ws=20):\n",
    "    \n",
    "    time_sec_var = -1.0\n",
    "    \n",
    "    val = 0\n",
    "    max_time = -1.0\n",
    "    \n",
    "    # for each sliding window           \n",
    "    for i in range(len(sec_var) - ws + 1):            \n",
    "        sec_var_mean = np.mean(sec_var[0+i:ws+i])\n",
    "        sec_var_std_dev = np.std(sec_var[0+i:ws+i])\n",
    "        \n",
    "        # check by sec var outliers\n",
    "        if abs(sec_var[ws+i-1] - sec_var_mean) > (n * sec_var_std_dev):\n",
    "            if time_sec_var == -1.0:\n",
    "                time_sec_var = (4+(ws+i-1)-1)/5\n",
    "                val = sec_var[ws+i-1]\n",
    "                break\n",
    "                \n",
    "    if val == 0:\n",
    "        val = max(sec_var)\n",
    "        max_time = (4 + sec_var.index(val) - 1)/5\n",
    "        \n",
    "    sec_var_hit = 0\n",
    "    \n",
    "    if time_sec_var != -1.0:\n",
    "        sec_var_hit = 1\n",
    "    \n",
    "    return sec_var_hit, time_sec_var, val, max_time\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrd(fsd1, fsd2):\n",
    "    MRD_nom = 0\n",
    "    MRD_denom = 0\n",
    "    for i in range(1, max(fsd1.keys())+1):\n",
    "        if i in fsd1.keys():\n",
    "            true = fsd1[i]\n",
    "        else:\n",
    "            true = 0\n",
    "            \n",
    "        if i in fsd2.keys():\n",
    "            est = fsd2[i]\n",
    "        else:\n",
    "            est = 0\n",
    "            \n",
    "        MRD_nom += abs(true - est)\n",
    "        MRD_denom += float(true + est)/2\n",
    "    MRD = MRD_nom/MRD_denom\n",
    "    \n",
    "    return MRD\n",
    "\n",
    "    # MRD_nom = 0\n",
    "    # MRD_denom = 0\n",
    "    # for key in fsd1.keys():\n",
    "    #     true = fsd1[key]\n",
    "    #     if key in fsd2.keys():\n",
    "    #         est = fsd2[key]\n",
    "    #     else:\n",
    "    #         est = 0\n",
    "            \n",
    "    #     MRD_nom += abs(true - est)\n",
    "    #     MRD_denom += float(true + est)/2\n",
    "    # MRD = MRD_nom/MRD_denom\n",
    "    \n",
    "    # return MRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fsd(algo='cm', row=3, width=4096, level=1, seed=1, count=1, flowkey='srcIP', \n",
    "              epochs=['10'], dataset='caida0517-125w_10_.pcap', window_size=200, dev=3.4, ws=25):\n",
    "    \n",
    "    # top 100\n",
    "    res_fsd = []\n",
    "    res_mrd = []\n",
    "    res_mrd_var = []\n",
    "    res_mrd_sec_var = []\n",
    "\n",
    "    answer = int(dataset.split('_')[1])\n",
    "    \n",
    "\n",
    "    res_mrd = [None, None] # 0, 1\n",
    "    res_mrd_var = [None, None, None] # 0, 1, 2\n",
    "    res_mrd_sec_var = [None, None, None, None] # 0, 1, 2, 3\n",
    "    \n",
    "    # fsd_total = read_fsd_data(algo, row, w, level, seed, count, flowkey, epochs, d, window_size)\n",
    "    fsd_total = read_single_window_fsd_data(algo, row, width, level, seed, count, flowkey, epochs, dataset, window_size)\n",
    "    # fsd_total = read_gt_fsd_data(algo, row, w, level, seed, count, flowkey, epochs, d, window_size)\n",
    "\n",
    "            \n",
    "    res_fsd = fsd_total[:-1] # ignore last window (less than 200ms)\n",
    "            \n",
    "    for i in range(1, len(res_fsd)):\n",
    "        res_mrd.append(calculate_mrd(res_fsd[i-1], res_fsd[i]))\n",
    "     \n",
    "    for i in range(3, len(res_mrd)):\n",
    "        res_mrd_var.append(abs(res_mrd[i] - res_mrd[i-1]))\n",
    "                \n",
    "    for i in range(4, len(res_mrd_var)):\n",
    "        res_mrd_sec_var.append(abs(res_mrd_var[i] - res_mrd_var[i-1]))\n",
    "    \n",
    "    sec_var_hit, time_sec_var, val, max_time = fsd_changes_occured(res_mrd_sec_var[4:], dev, ws)\n",
    "    \n",
    "    change = 0\n",
    "    changing_time = 0.0\n",
    "    if sec_var_hit == 1:\n",
    "        change = 1\n",
    "        changing_time = time_sec_var\n",
    "\n",
    "    return change, changing_time, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.0, 10)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_fsd(dataset=pcap_file[0], window_size=200, dev=3.4, ws=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mrd(file_name, mrd_list, window_size, predict_time, ans):\n",
    "    \n",
    "    res = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(mrd_list[0])):\n",
    "        single_mrd = []\n",
    "        for j in range(len(mrd_list)):\n",
    "            single_mrd.append(mrd_list[j][i])\n",
    "        res.append(single_mrd)\n",
    "        \n",
    "    profiler_folder_path = \"/home/ming/SketchMercator/pattern_detection/traffic_generator/pcap_file/\"\n",
    "    for name in sorted(os.listdir(profiler_folder_path)):\n",
    "        if name.split(\"-\")[0] == \"zipf4\":\n",
    "            continue\n",
    "        labels.append(name.split(\"_\")[0])    \n",
    "        \n",
    "        \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    for i in range(0, len(res)):\n",
    "        if labels[i] == ans:\n",
    "            plt.plot(res[i], label=labels[i], color='tab:red', linewidth=3.0)\n",
    "        else:\n",
    "            plt.plot(res[i], label=labels[i])\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Time (sec)')\n",
    "    plt.ylabel('MRD')\n",
    "    plt.title('%s ,Random K MRD Variation' % (file_name))\n",
    "    # plt.axhline(10000, c=\"black\")\n",
    "    ticks = [i for i in range(int(predict_time*1000/window_size) + 1)]\n",
    "    adjusted_ticks = [tick * (window_size / 1000) for tick in ticks[0::int(1000 / window_size)]]\n",
    "    plt.xticks(ticks[0::int(1000/window_size)], adjusted_ticks)\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distribution(profiler_fsd_dict, flowkey_list, unknown_fsd):\n",
    "    # get profiler dist\n",
    "    profiler_fsd = {}\n",
    "    for name, map in profiler_fsd_dict.items():\n",
    "        profiler_fsd[name] = get_pdf(get_profiler_sampled_fsd(map, flowkey_list))\n",
    "        \n",
    "    all_mrd = {}\n",
    "    min_mrd = sys.float_info.max\n",
    "    predict_dist = \"\"\n",
    "    for name, dist in profiler_fsd.items():\n",
    "        if dist == {}:\n",
    "            # print(name, \"is empty\")\n",
    "            mrd = abs(calculate_mrd(get_pdf(unknown_fsd), dist))\n",
    "            # continue\n",
    "        else:\n",
    "            mrd = calculate_mrd(dist, get_pdf(unknown_fsd))\n",
    "        all_mrd[name] = mrd\n",
    "        if mrd < min_mrd:\n",
    "            min_mrd = mrd\n",
    "            predict_dist = name\n",
    "    # for name, dist in profiler_fsd.items():\n",
    "    #     mrd = calculate_mrd(dist, unknown_fsd)\n",
    "    #     all_mrd[name] = mrd\n",
    "    #     if mrd < min_mrd:\n",
    "    #         min_mrd = mrd\n",
    "    #         predict_dist = name\n",
    "           \n",
    "    # print(all_mrd)\n",
    "    # print(predict_dist)       \n",
    "    \n",
    "    mrds = []\n",
    "    for name, mrd in all_mrd.items():\n",
    "        mrds.append(mrd)\n",
    "    \n",
    "    # print(len(mrds))\n",
    "    # return predict_dist, profiler_fsd[predict_dist]\n",
    "    return predict_dist, mrds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_distribution(algo='cm', row=3, width=4096, level=1, seed=1, count=1, flowkey='srcIP', \n",
    "              epochs=['10'], dataset='caida0517-125w_10_.pcap', window_size=200, start_time=0, predict_length=2):\n",
    "    \n",
    "    res_fsd = []\n",
    "    \n",
    "    answer = int(dataset.split('_')[1])\n",
    "    \n",
    "    # fsd_total = read_fsd_data(algo, row, width, level, seed, count, flowkey, epochs, dataset, window_size)\n",
    "    # fsd_total = read_single_window_fsd_data(algo, row, width, level, seed, count, flowkey, epochs, dataset, window_size)\n",
    "    fsd_total = read_single_window_gt_fsd_data(algo, row, width, level, seed, count, flowkey, epochs, dataset, window_size)\n",
    "    # fsd_total = read_gt_fsd_data(algo, row, w, level, seed, count, flowkey, epochs, d, window_size)\n",
    "    \n",
    "    res_fsd = fsd_total[start_time:]\n",
    "            \n",
    "    # find dist\n",
    "    profiler_fsd_dict = prepare_profiler_final_fsd_dict()\n",
    "    sampled_flowkey = get_sampled_flowkey(dataset=dataset, predict_time=0)[start_time:]\n",
    "    \n",
    "    print(len(res_fsd), len(sampled_flowkey))\n",
    "    \n",
    "    vote = {}\n",
    "    all_mrds = []\n",
    "    for j in range(min(len(res_fsd), len(sampled_flowkey), int(predict_length * 1000 / window_size))):\n",
    "        dist_name, mrds = find_distribution(profiler_fsd_dict, sampled_flowkey[j], res_fsd[j])\n",
    "        all_mrds.append(mrds)\n",
    "        \n",
    "        if dist_name[:-9] in vote:\n",
    "            vote[dist_name[:-9]] += 1\n",
    "        else:\n",
    "            vote[dist_name[:-9]] = 1\n",
    "            \n",
    "    if vote == {}:\n",
    "        return \"NONE\"\n",
    "    return max(vote, key=vote.get)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ming/SketchMercator/')\n",
    "\n",
    "from sw_dp_simulator.file_io.py.read_cm import load_cm\n",
    "from sw_dp_simulator.hash_module.py.hash import compute_hash\n",
    "from sw_dp_simulator.file_io.py.common import parse_line\n",
    "\n",
    "def counter_estimate(key, sketch_array, index_hash_sub_list, d, w, hash, level):\n",
    "    a = []\n",
    "\n",
    "    for i in range(0, d):\n",
    "        index = compute_hash(key, hash, index_hash_sub_list[i], w)\n",
    "        estimate = sketch_array[i * w + index]\n",
    "        a.append(estimate)\n",
    "\n",
    "    return min(a)\n",
    "\n",
    "def get_counter_value(full_dir, row, width, level, window_size):\n",
    "    counter_list = []\n",
    "    \n",
    "    # load counter value\n",
    "    for l in range(0, 1):\n",
    "        window_dir = '%s/level_%02d/window_%d/' % (full_dir, l, window_size)\n",
    "        print(window_dir)\n",
    "        \n",
    "        level_list = []\n",
    "        for file in sorted(os.listdir(window_dir)):\n",
    "            window_list = []\n",
    "            path = os.path.join(window_dir, file)\n",
    "            \n",
    "            f = open(path)\n",
    "            for i in range(0, row * width):\n",
    "                pline = f.readline().strip()\n",
    "                window_list.append(int(pline))\n",
    "            f.close()\n",
    "            \n",
    "            level_list.append(window_list)\n",
    "            \n",
    "        final_counter_path = '%s/level_%02d/sketch_counter.txt' % (full_dir, l)\n",
    "        window_list = []\n",
    "        f = open(final_counter_path)\n",
    "        for i in range(0, row * width):\n",
    "            pline = f.readline().strip()\n",
    "            window_list.append(int(pline))\n",
    "        f.close()\n",
    "        \n",
    "        level_list.append(window_list)\n",
    "        \n",
    "        print(f'There are {len(level_list)} windows')\n",
    "        \n",
    "        counter_list.append(level_list)\n",
    "    \n",
    "    return counter_list\n",
    "\n",
    "def get_topk_flowkey(full_dir, row, width, level, window_size, k):\n",
    "    key_list = []\n",
    "    \n",
    "    # load counter value\n",
    "    for l in range(0, 1):\n",
    "        key_window_dir = '%s/level_%02d/key_window_%d/' % (full_dir, l, window_size)\n",
    "        print(key_window_dir)\n",
    "        \n",
    "        level_list = []\n",
    "        for file in sorted(os.listdir(key_window_dir)):\n",
    "            key_window_list = []\n",
    "            path = os.path.join(key_window_dir, file)\n",
    "            \n",
    "            f = open(path)\n",
    "            key = f.readline().strip()\n",
    "            # print(key)\n",
    "            for line in f:\n",
    "                string_key, estimate, flowkey = parse_line(key, line.strip())\n",
    "                key_window_list.append(flowkey)\n",
    "            f.close()\n",
    "            \n",
    "            level_list.append(key_window_list)\n",
    "            \n",
    "        final_counter_path = '%s/flowkey.txt' % (full_dir)\n",
    "        cnt = 0\n",
    "        key_window_list = []\n",
    "        f = open(final_counter_path)\n",
    "        key = f.readline().strip()\n",
    "        # print(key)\n",
    "        for line in f:\n",
    "            string_key, estimate, flowkey = parse_line(key, line.strip())\n",
    "            key_window_list.append(flowkey)\n",
    "        f.close()\n",
    "        \n",
    "        # random sample k flow key\n",
    "        if k != 0:\n",
    "            random.shuffle(key_window_list)\n",
    "            \n",
    "            level_list.append(key_window_list[:k])\n",
    "        else:\n",
    "            level_list.append(key_window_list)\n",
    "            \n",
    "        print(f'There are {len(level_list)} windows')\n",
    "        \n",
    "        key_list.append(level_list)\n",
    "    \n",
    "    return key_list\n",
    "\n",
    "def read_period_fsd_data(algo='cm', row=3, width=4096, level=1, seed=1, count=1, flowkey='srcIP', \n",
    "              epochs=['10'], dataset='caida0517-125w_10_.pcap', window_size=200, start=30, length=3):\n",
    "    \n",
    "    fs_dist = {}\n",
    "    for epoch in epochs:\n",
    "        \n",
    "        path = f\"../SketchPadding/{algo}/{dataset}/\"\\\n",
    "                f\"{flowkey}/row_{row}_width_{width}_level_{level}_epoch_{epoch}_count_{count}_seed_{seed}/\"\n",
    "        \n",
    "        for dir in sorted(os.listdir(path)):\n",
    "            p = os.path.join(path, dir)\n",
    "            if os.path.isdir(p): \n",
    "                \n",
    "                result = load_cm(p, width, row)\n",
    "                index_hash_list = result[\"index_hash_list\"]\n",
    "                \n",
    "                counter_list = get_counter_value(p, row, width, level, window_size)\n",
    "                \n",
    "                cArray_start = counter_list[0][start]\n",
    "                cArray_end = counter_list[0][min(len(counter_list[0])-1, start + int(length*1000/window_size))]\n",
    "                \n",
    "                randomk_flowkey_list = get_topk_flowkey(p, row, width, level, window_size, 5000)\n",
    "                \n",
    "                for key in randomk_flowkey_list[0][start + int(length*1000/window_size)]:\n",
    "                    est_start = counter_estimate(key, cArray_start, index_hash_list[0], row, width, \"crc_hash\", 0)\n",
    "                    est_end = counter_estimate(key, cArray_end, index_hash_list[0], row, width, \"crc_hash\", 0)\n",
    "                    \n",
    "                    var = est_end - est_start\n",
    "                    if var == 0:\n",
    "                        continue\n",
    "                    if var in fs_dist.keys(): \n",
    "                        fs_dist[var] += 1\n",
    "                    else:\n",
    "                        fs_dist[var] = 1\n",
    "                        \n",
    "                fs_dist = dict(sorted(fs_dist.items()))\n",
    "            \n",
    "    return fs_dist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_period_distribution(algo='cm', row=3, width=4096, level=1, seed=1, count=1, flowkey='srcIP', \n",
    "              epochs=['10'], dataset='caida0517-125w_10_.pcap', window_size=200, start_time=0, predict_length=2):\n",
    "    \n",
    "    res_fsd = []\n",
    "    \n",
    "    answer = int(dataset.split('_')[1])\n",
    "    \n",
    "    # fsd_total = read_fsd_data(algo, row, width, level, seed, count, flowkey, epochs, dataset, window_size)\n",
    "    fsd_total = read_single_window_fsd_data(algo, row, width, level, seed, count, flowkey, epochs, dataset, window_size)\n",
    "    # fsd_total = read_gt_fsd_data(algo, row, w, level, seed, count, flowkey, epochs, d, window_size)\n",
    "    \n",
    "    res_fsd = fsd_total[start_time:]\n",
    "            \n",
    "    # find dist\n",
    "    profiler_fsd_dict = prepare_profiler_final_fsd_dict()\n",
    "    sampled_flowkey = get_sampled_flowkey(dataset=dataset, predict_time=0)\n",
    "    if start_time >= len(sampled_flowkey):\n",
    "        return \"NONE\"\n",
    "    \n",
    "    end_sampled_flowkey = sampled_flowkey[min(len(sampled_flowkey)-1,start_time+int(predict_length*1000/window_size))]\n",
    "    res_fsd = read_period_fsd_data(algo, row, width, level, seed, count, flowkey, epochs, dataset, window_size, start_time, predict_length)\n",
    "    \n",
    "    # print(len(res_fsd), len(sampled_flowkey))\n",
    "    \n",
    "    dist_name, mrds = find_distribution(profiler_fsd_dict, end_sampled_flowkey, res_fsd)\n",
    "    \n",
    "    return dist_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'caida0517-500w'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_distribution(width=widths[0], dataset=pcap_file[0], window_size=200, start_time=0, predict_length=2)\n",
    "# predict_period_distribution(width=widths[0], dataset=pcap_file[0], window_size=200, start_time=30, predict_length=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict PCD - TFS\n",
    "---\n",
    "- Start by detecting changes in TFS:\n",
    "    - If TFS changed  Measure current DIST (2s).\n",
    "    - If TFS unchanged  Check if FSD has changed:\n",
    "        - If FSD changed  Measure current DIST (2s).\n",
    "- Once the current DIST is known:\n",
    "    - Select an appropriate profiler based on collected predict TFS (2s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pcd_tfs(algo='cm', row=3, width=[4096], level=1, seed=1, count=1, flowkey='srcIP', \n",
    "              epochs=['10'], datasets=['caida0517-125w_10_.pcap'], window_size=200, ws=25):\n",
    "    \n",
    "    TFS_CHANGE = {}\n",
    "    TFS_TIME = {}\n",
    "    TFS_VAL = {}\n",
    "    \n",
    "    FSD_CHANGE = {}\n",
    "    FSD_TIME = {}\n",
    "    FSD_VAL = {}\n",
    "    \n",
    "    for d in datasets:\n",
    "        for w in width:\n",
    "            name = f'{d[:-5]}_{w}'\n",
    "            \n",
    "            # detect TFS changes\n",
    "            tfs_dev = 3\n",
    "            tfs_change, tfs_changing_time, answer = predict_tfs(algo, row, w, level, seed, count, flowkey, epochs, d, window_size, tfs_dev, ws)\n",
    "            \n",
    "            if answer == 10:\n",
    "                if tfs_change == 0:\n",
    "                    print(f'{d} [tfs correct] {answer}')\n",
    "                    TFS_CHANGE[name] = 0\n",
    "                else:\n",
    "                    print(f\"{d} [tfs error] {tfs_changing_time} {answer}\")\n",
    "                    TFS_CHANGE[name] = 1\n",
    "                    TFS_TIME[name] = int(tfs_changing_time/(window_size/1000))\n",
    "            else:\n",
    "                if tfs_change == 0:\n",
    "                    print(f\"{d} [tfs error] {tfs_changing_time} {answer}\")\n",
    "                    TFS_CHANGE[name] = 0\n",
    "                elif tfs_change == 1:\n",
    "                    if abs(tfs_changing_time - answer) >= 1.0: # predict failed\n",
    "                        print(f\"{d} [tfs error] {tfs_changing_time} {answer}\")\n",
    "                        TFS_CHANGE[name] = 1\n",
    "                        TFS_TIME[name] = int(tfs_changing_time/(window_size/1000))\n",
    "                    else:\n",
    "                        print(f'{d} [tfs correct] {tfs_changing_time} {answer}')\n",
    "                        TFS_CHANGE[name] = 1\n",
    "                        TFS_TIME[name] = int(tfs_changing_time/(window_size/1000))\n",
    "                        \n",
    "            # detect FSD changes\n",
    "            fsd_dev = 3.4\n",
    "            fsd_change, fsd_changing_time, answer = predict_fsd(algo, row, w, level, seed, count, flowkey, epochs, d, window_size, fsd_dev, ws)\n",
    "            \n",
    "            if answer == 10:\n",
    "                if fsd_change == 0:\n",
    "                    print(f'{d} [fsd correct] {answer}')\n",
    "                    FSD_CHANGE[name] = 0\n",
    "                else:\n",
    "                    print(f\"{d} [fsd error] {fsd_changing_time} {answer}\")\n",
    "                    FSD_CHANGE[name] = 1\n",
    "                    FSD_TIME[name] = int(fsd_changing_time/(window_size/1000))\n",
    "            else:\n",
    "                if fsd_change == 0:\n",
    "                    print(f\"{d} [fsd error] {fsd_changing_time} {answer}\")\n",
    "                    FSD_CHANGE[name] = 0\n",
    "                elif fsd_change == 1:\n",
    "                    if abs(fsd_changing_time - answer) >= 1.0: # predict failed\n",
    "                        print(f\"{d} [fsd error] {fsd_changing_time} {answer}\")\n",
    "                        FSD_CHANGE[name] = 1\n",
    "                        FSD_TIME[name] = int(fsd_changing_time/(window_size/1000))\n",
    "                    else:\n",
    "                        print(f'{d} [fsd correct] {fsd_changing_time} {answer}')\n",
    "                        FSD_CHANGE[name] = 1\n",
    "                        FSD_TIME[name] = int(fsd_changing_time/(window_size/1000))\n",
    "                            \n",
    "            # one of which changes -> select profiler\n",
    "            predict_length = 3\n",
    "            # if TFS_CHANGE[name] == 1 and FSD_CHANGE[name] == 0:\n",
    "            #     TFS_VAL[name] = predict_tfs_val(algo, row, w, level, seed, count, flowkey, epochs, d, window_size, TFS_TIME[name], predict_length)\n",
    "            #     FSD_VAL[name] = predict_distribution(algo, row, w, level, seed, count, flowkey, epochs, d, window_size, TFS_TIME[name], predict_length)\n",
    "            # elif TFS_CHANGE[name] == 0 and FSD_CHANGE[name] == 1:\n",
    "            #     TFS_VAL[name] = predict_tfs_val(algo, row, w, level, seed, count, flowkey, epochs, d, window_size, FSD_TIME[name], predict_length)\n",
    "            #     FSD_VAL[name] = predict_distribution(algo, row, w, level, seed, count, flowkey, epochs, d, window_size, FSD_TIME[name], predict_length)\n",
    "            # elif TFS_CHANGE[name] == 1 and FSD_CHANGE[name] == 1:\n",
    "            #     changing_time = min(TFS_TIME[name], FSD_TIME[name]) # once the changing point is detect => make a prediction\n",
    "            #     TFS_VAL[name] = predict_tfs_val(algo, row, w, level, seed, count, flowkey, epochs, d, window_size, changing_time, predict_length)\n",
    "            #     FSD_VAL[name] = predict_distribution(algo, row, w, level, seed, count, flowkey, epochs, d, window_size, changing_time, predict_length)\n",
    "            # else:\n",
    "            #     print(\"no need to re-profiling\")\n",
    "            \n",
    "            T = int(answer/(window_size/1000))\n",
    "            \n",
    "            if TFS_CHANGE[name] == 1 and FSD_CHANGE[name] == 0:\n",
    "                TFS_VAL[name] = predict_tfs_val(algo, row, w, level, seed, count, flowkey, epochs, d, window_size, T, predict_length)\n",
    "                FSD_VAL[name] = predict_distribution(algo, row, w, level, seed, count, flowkey, epochs, d, window_size, T, predict_length)\n",
    "            elif TFS_CHANGE[name] == 0 and FSD_CHANGE[name] == 1:\n",
    "                TFS_VAL[name] = predict_tfs_val(algo, row, w, level, seed, count, flowkey, epochs, d, window_size, T, predict_length)\n",
    "                FSD_VAL[name] = predict_distribution(algo, row, w, level, seed, count, flowkey, epochs, d, window_size, T, predict_length)\n",
    "            elif TFS_CHANGE[name] == 1 and FSD_CHANGE[name] == 1:\n",
    "                TFS_VAL[name] = predict_tfs_val(algo, row, w, level, seed, count, flowkey, epochs, d, window_size, T, predict_length)\n",
    "                FSD_VAL[name] = predict_distribution(algo, row, w, level, seed, count, flowkey, epochs, d, window_size, T, predict_length)\n",
    "            else:\n",
    "                print(\"no need to re-profiling\")\n",
    "                    \n",
    "                \n",
    "    print(\"-- result --\")\n",
    "    correct = 0\n",
    "    half_correct = 0\n",
    "    cnt = 0\n",
    "    for d in datasets:\n",
    "        for w in width:\n",
    "            name = f'{d[:-5]}_{w}'\n",
    "            \n",
    "            print(cnt)\n",
    "            cnt += 1\n",
    "            origin = d.split(\"_\")[0]\n",
    "            after = d.split(\"_\")[0]\n",
    "            if len(d.split(\"_\")) == 4:\n",
    "                after = d.split(\"_\")[2]\n",
    "            print(f'\\tORIGIN: {origin}')\n",
    "            print(f'\\tAFTER : {after}')\n",
    "                \n",
    "            if TFS_CHANGE[name] == 1 or FSD_CHANGE[name] == 1:\n",
    "                print(f'\\tpredict TFS = {TFS_VAL[name]}, predict FSD = {FSD_VAL[name]}')\n",
    "                \n",
    "                if FSD_VAL[name] == after:\n",
    "                    correct += 1\n",
    "                elif origin == after and FSD_VAL[name] == \"NONE\":\n",
    "                        correct += 1\n",
    "                        \n",
    "                if FSD_VAL[name].split(\"-\")[0] == after.split(\"-\")[0]:\n",
    "                    half_correct += 1\n",
    "                elif origin.split(\"-\")[0] == after.split(\"-\")[0] and FSD_VAL[name] == \"NONE\":\n",
    "                    half_correct += 1\n",
    "            else: # both TFS and FSD no change => no trigger re-profiling\n",
    "                print(\"\\tno need to re-profiling\")\n",
    "                if origin == after:\n",
    "                    correct += 1\n",
    "                if origin.split(\"-\")[0] == after.split(\"-\")[0]:\n",
    "                    half_correct += 1\n",
    "                    \n",
    "    print(f'   correct   predict: {correct}/{len(datasets)*len(width)} {correct*100/(len(datasets)*len(width))}%')\n",
    "    print(f'half_correct predict: {half_correct}/{len(datasets)*len(width)} {half_correct*100/(len(datasets)*len(width))}%')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caida0517-500w_6_caida0816-600w_4.pcap [tfs error] 9.4 6\n",
      "caida0517-500w_6_caida0816-600w_4.pcap [fsd error] 0.0 6\n",
      "18 17\n",
      "caida0517-500w_6_caida0816-300w_4.pcap [tfs correct] 5.6 6\n",
      "caida0517-500w_6_caida0816-300w_4.pcap [fsd correct] 5.6 6\n",
      "18 17\n",
      "caida0517-500w_6_caida0816-150w_4.pcap [tfs correct] 5.4 6\n",
      "caida0517-500w_6_caida0816-150w_4.pcap [fsd correct] 5.6 6\n",
      "18 17\n",
      "caida0517-250w_6_caida0816-600w_4.pcap [tfs correct] 5.4 6\n",
      "caida0517-250w_6_caida0816-600w_4.pcap [fsd correct] 5.4 6\n",
      "18 17\n",
      "caida0517-250w_6_caida0816-300w_4.pcap [tfs error] 9.4 6\n",
      "caida0517-250w_6_caida0816-300w_4.pcap [fsd correct] 5.4 6\n",
      "18 17\n",
      "caida0517-250w_6_caida0816-150w_4.pcap [tfs correct] 5.6 6\n",
      "caida0517-250w_6_caida0816-150w_4.pcap [fsd correct] 5.6 6\n",
      "18 17\n",
      "caida0517-125w_6_caida0816-600w_4.pcap [tfs correct] 5.4 6\n",
      "caida0517-125w_6_caida0816-600w_4.pcap [fsd correct] 5.4 6\n",
      "18 17\n",
      "caida0517-125w_6_caida0816-300w_4.pcap [tfs correct] 5.4 6\n",
      "caida0517-125w_6_caida0816-300w_4.pcap [fsd correct] 5.4 6\n",
      "18 17\n",
      "caida0517-125w_6_caida0816-150w_4.pcap [tfs error] 7.0 6\n",
      "caida0517-125w_6_caida0816-150w_4.pcap [fsd correct] 5.4 6\n",
      "18 17\n",
      "caida0816-600w_6_caida0517-500w_4.pcap [tfs correct] 6.4 6\n",
      "caida0816-600w_6_caida0517-500w_4.pcap [fsd correct] 6.0 6\n",
      "18 17\n",
      "caida0816-600w_6_caida0517-250w_4.pcap [tfs correct] 6.0 6\n",
      "caida0816-600w_6_caida0517-250w_4.pcap [fsd correct] 6.0 6\n",
      "18 17\n",
      "caida0816-600w_6_caida0517-125w_4.pcap [tfs correct] 6.0 6\n",
      "caida0816-600w_6_caida0517-125w_4.pcap [fsd correct] 6.0 6\n",
      "18 17\n",
      "caida0816-300w_6_caida0517-500w_4.pcap [tfs correct] 6.0 6\n",
      "caida0816-300w_6_caida0517-500w_4.pcap [fsd correct] 6.0 6\n",
      "18 17\n",
      "caida0816-300w_6_caida0517-250w_4.pcap [tfs correct] 6.4 6\n",
      "caida0816-300w_6_caida0517-250w_4.pcap [fsd correct] 6.0 6\n",
      "18 17\n",
      "caida0816-300w_6_caida0517-125w_4.pcap [tfs correct] 6.0 6\n",
      "caida0816-300w_6_caida0517-125w_4.pcap [fsd correct] 6.0 6\n",
      "18 17\n",
      "caida0816-150w_6_caida0517-500w_4.pcap [tfs correct] 6.0 6\n",
      "caida0816-150w_6_caida0517-500w_4.pcap [fsd correct] 6.0 6\n",
      "18 17\n",
      "caida0816-150w_6_caida0517-250w_4.pcap [tfs correct] 6.0 6\n",
      "caida0816-150w_6_caida0517-250w_4.pcap [fsd correct] 6.0 6\n",
      "18 17\n",
      "caida0816-150w_6_caida0517-125w_4.pcap [tfs correct] 6.4 6\n",
      "caida0816-150w_6_caida0517-125w_4.pcap [fsd correct] 6.0 6\n",
      "18 17\n",
      "-- result --\n",
      "0\n",
      "\tORIGIN: caida0517-500w\n",
      "\tAFTER : caida0816-600w\n",
      "\tpredict TFS = 5824943, predict FSD = zipf2b-400w\n",
      "1\n",
      "\tORIGIN: caida0517-500w\n",
      "\tAFTER : caida0816-300w\n",
      "\tpredict TFS = 2941786, predict FSD = zipf2a-150w\n",
      "2\n",
      "\tORIGIN: caida0517-500w\n",
      "\tAFTER : caida0816-150w\n",
      "\tpredict TFS = 1578163, predict FSD = zipf2a-150w\n",
      "3\n",
      "\tORIGIN: caida0517-250w\n",
      "\tAFTER : caida0816-600w\n",
      "\tpredict TFS = 5824936, predict FSD = zipf2b-400w\n",
      "4\n",
      "\tORIGIN: caida0517-250w\n",
      "\tAFTER : caida0816-300w\n",
      "\tpredict TFS = 2941800, predict FSD = zipf2a-150w\n",
      "5\n",
      "\tORIGIN: caida0517-250w\n",
      "\tAFTER : caida0816-150w\n",
      "\tpredict TFS = 1578226, predict FSD = zipf2a-150w\n",
      "6\n",
      "\tORIGIN: caida0517-125w\n",
      "\tAFTER : caida0816-600w\n",
      "\tpredict TFS = 5825033, predict FSD = zipf2b-400w\n",
      "7\n",
      "\tORIGIN: caida0517-125w\n",
      "\tAFTER : caida0816-300w\n",
      "\tpredict TFS = 2941896, predict FSD = zipf2a-150w\n",
      "8\n",
      "\tORIGIN: caida0517-125w\n",
      "\tAFTER : caida0816-150w\n",
      "\tpredict TFS = 1578343, predict FSD = zipf2a-150w\n",
      "9\n",
      "\tORIGIN: caida0816-600w\n",
      "\tAFTER : caida0517-500w\n",
      "\tpredict TFS = 5258973, predict FSD = zipf2a-150w\n",
      "10\n",
      "\tORIGIN: caida0816-600w\n",
      "\tAFTER : caida0517-250w\n",
      "\tpredict TFS = 2567063, predict FSD = zipf2a-150w\n",
      "11\n",
      "\tORIGIN: caida0816-600w\n",
      "\tAFTER : caida0517-125w\n",
      "\tpredict TFS = 1286633, predict FSD = zipf2a-150w\n",
      "12\n",
      "\tORIGIN: caida0816-300w\n",
      "\tAFTER : caida0517-500w\n",
      "\tpredict TFS = 5258713, predict FSD = zipf2a-150w\n",
      "13\n",
      "\tORIGIN: caida0816-300w\n",
      "\tAFTER : caida0517-250w\n",
      "\tpredict TFS = 2566916, predict FSD = zipf2a-150w\n",
      "14\n",
      "\tORIGIN: caida0816-300w\n",
      "\tAFTER : caida0517-125w\n",
      "\tpredict TFS = 1286560, predict FSD = zipf2a-150w\n",
      "15\n",
      "\tORIGIN: caida0816-150w\n",
      "\tAFTER : caida0517-500w\n",
      "\tpredict TFS = 5258526, predict FSD = zipf2a-150w\n",
      "16\n",
      "\tORIGIN: caida0816-150w\n",
      "\tAFTER : caida0517-250w\n",
      "\tpredict TFS = 2566820, predict FSD = zipf2a-150w\n",
      "17\n",
      "\tORIGIN: caida0816-150w\n",
      "\tAFTER : caida0517-125w\n",
      "\tpredict TFS = 1286500, predict FSD = zipf2a-150w\n",
      "   correct   predict: 0/18 0.0%\n",
      "half_correct predict: 0/18 0.0%\n"
     ]
    }
   ],
   "source": [
    "# predict_pcd_tfs(width=widths, datasets=pcap_file[0:12])\n",
    "predict_pcd_tfs(width=widths, datasets=pcap_file[12:30])\n",
    "# predict_pcd_tfs(width=widths, datasets=pcap_file[30:48])\n",
    "# predict_pcd_tfs(width=widths, datasets=pcap_file[48:66])\n",
    "# predict_pcd_tfs(width=widths, datasets=pcap_file[66:84])\n",
    "# predict_pcd_tfs(width=widths, datasets=pcap_file[84:102])\n",
    "# predict_pcd_tfs(width=widths, datasets=pcap_file[102:120])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
